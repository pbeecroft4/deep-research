{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxATavCROk-U"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/morganmcg1/deep-research-bot/blob/main/notebooks/02_deep_research_agent.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc7GKwVJ23R6"
      },
      "source": [
        "# Building a Deep Research Agent\n",
        "\n",
        "<!--- @wandbcode{fc-london-workshop-2025} -->\n",
        "\n",
        "This script will walk you through building on top of our simple tool calling agent to evolve it to a full Deep Research Agent.\n",
        "We will cover:\n",
        "1. Prompting strategies\n",
        "2. Multi-tool agents design\n",
        "3. Compacting conversations\n",
        "\n",
        "Docs:\n",
        "- Weights & Biases Inference [docs](https://docs.wandb.ai/inference)\n",
        "- Weave [docs](https://docs.wandb.ai/weave)\n",
        "- Exa [docs](https://docs.exa.ai/reference/search)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1huUI69O3bx"
      },
      "source": [
        "## Imports + API keys\n",
        "\n",
        "Our Deep Research Agent will actually still only use 2 services:\n",
        "1. W&B for inference and tracking\n",
        "2. Exa for web search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIFrSTlXOk-X"
      },
      "outputs": [],
      "source": [
        "#Make sure you have the necessary libraries installed:\n",
        "!uv pip install -qqq git+https://github.com/morganmcg1/deep-research-bot.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YAYV7FmTOk-Y"
      },
      "outputs": [],
      "source": [
        "# In this notebook we will use the `uv` package to install the dependencies. github: https://github.com/astral-sh/uv\n",
        "# If you do not have UV installed, you can uncomment and run:\n",
        "# On macOS and Linux.\n",
        "#!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
        "\n",
        "# On Windows.\n",
        "#!powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BEiKMVKS8slD",
        "outputId": "62fd2729-af12-4c2a-c6a9-36117fe7f6e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'weave'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2350228889.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mweave\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpydantic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'weave'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Global Configuration & Setup\n",
        "import json\n",
        "import os\n",
        "import weave\n",
        "import openai\n",
        "from pydantic import Field\n",
        "from typing import Any\n",
        "from exa_py import Exa\n",
        "from datetime import datetime\n",
        "\n",
        "from deep_research_bot.utils import console"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXYfji9jOk-Y"
      },
      "source": [
        "### Add your API Keys\n",
        "We'll need a wandb api key and an exa api key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXzDaq29Ok-Z"
      },
      "outputs": [],
      "source": [
        "os.environ[\"WANDB_API_KEY\"] = \"\"\n",
        "os.environ[\"EXA_API_KEY\"] = \"\"\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ[\"WANDB_API_KEY\"] = userdata.get('WANDB_API_KEY')\n",
        "os.environ[\"EXA_API_KEY\"] = userdata.get('EXA_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29IL8eNR73wn"
      },
      "outputs": [],
      "source": [
        "MODEL_SMALL = \"Qwen/Qwen3-235B-A22B-Instruct-2507\"\n",
        "MODEL_MEDIUM = \"zai-org/GLM-4.5\"\n",
        "MODEL_LARGE = \"moonshotai/Kimi-K2-Instruct\"\n",
        "MODEL_SMALL_CONTEXT = \"OpenPipe/Qwen3-14B-Instruct\"\n",
        "\n",
        "WANDB_ENTITY = \"\"\n",
        "WANDB_PROJECT = \"london-workshop-2025\"\n",
        "\n",
        "os.environ[\"WANDB_ENTITY\"] = WANDB_ENTITY\n",
        "\n",
        "oai_client = openai.OpenAI(\n",
        "    base_url='https://api.inference.wandb.ai/v1',\n",
        "    api_key=os.getenv(\"WANDB_API_KEY\"),\n",
        "    project=f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")\n",
        "\n",
        "exa_client = Exa(api_key=os.getenv(\"EXA_API_KEY\")   )\n",
        "\n",
        "weave.init(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcFtYhvCO9Bw"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1oA5aMwOk-Z"
      },
      "outputs": [],
      "source": [
        "# these are the same functions we have covered in the notebook 01_simple_tool_calling_agent.ipynb so lets just import them here\n",
        "from deep_research_bot.utils import function_tool, perform_tool_calls\n",
        "\n",
        "# this is the same call_model function from notebook 01_simple_tool_calling_agent.ipynb\n",
        "@weave.op\n",
        "def call_model(model_name: str, messages: list[dict[str, Any]], **kwargs) -> str:\n",
        "    \"Call a model with the given messages and kwargs.\"\n",
        "    response = oai_client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=messages,\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message\n",
        "\n",
        "# new, simple function to get the current date which can help the agent ground the research in the current time\n",
        "def get_today_str() -> str:\n",
        "    \"\"\"Get current date in a human-readable format.\"\"\"\n",
        "    return datetime.now().strftime(\"%a %b %-d, %Y\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH2oK-UlTaOp"
      },
      "source": [
        "## Prompts\n",
        "\n",
        "When writing a prompt for a Deep Research Agent you should still follow the same principles as for any other LLM prompt by giving it its role, the task and well formating the whole prompt.\n",
        "What is different is the list and description of the tools available to the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pr67sUm2TZmk"
      },
      "outputs": [],
      "source": [
        "DEEP_RESEARCH_AGENT_PROMPT = \"\"\"\n",
        "  You are a research assistant conducting research on the user's input topic. For context, today's date is {date}.                                                                                                        ‚îÇ\n",
        "\n",
        "  <Task>\n",
        "  Your job is to use tools to gather information about the user's input topic and write a blog post as an answer.\n",
        "  You can use any of the tools provided to you to find resources that can help answer the research question.\n",
        "  You can call these tools in series or in parallel, your research is conducted in a tool-calling loop.\n",
        "  Your response should be a thorough answer to the user's question, citing sources and reasoning, providing an overview of the facts or any gaps in the subject.\n",
        "  </Task>\n",
        "\n",
        "  <Available Tools>\n",
        "  You have access to the following tools:\n",
        "  1. **clarification_tool**: For asking user clarifying questions if needed. If you have clarifying questions start with this.\n",
        "  2. **planning_tool**: For planning the research.\n",
        "  2. **exa_search_and_refine**: For conducting web searches to gather information\n",
        "  2. **think_tool**: For reflection and strategic planning during research\n",
        "\n",
        "  **CRITICAL: Use think_tool after each search to reflect on results and plan next steps**\n",
        "  </Available Tools>\n",
        "\n",
        "  <Instructions>\n",
        "  Think like a human researcher with limited time. Follow these steps:\n",
        "  1. **Read the question carefully** - What specific information does the user need?\n",
        "  2. **Start with broader searches** - Use broad, comprehensive queries first\n",
        "  3. **After each search, pause and assess** - Do I have enough to answer? What's still missing?\n",
        "  4. **Execute narrower searches as you gather information** - Fill in the gaps\n",
        "  5. **Stop when you can answer confidently** - Don't keep searching for perfection\n",
        "  6. **Provide an answer** - At the end, always provide the answer from your research.\n",
        "  7. **Write a blog post style answer** - Write a blog post style answer that is indepth, well structured,easy to understand and engaging.\n",
        "  </Instructions>\n",
        "\n",
        "  **Stop Immediately When**:\n",
        "  - You can answer the user's question comprehensively\n",
        "  - You have 3+ relevant examples/sources for the question\n",
        "  - Your last 2 searches returned similar information\n",
        "  </Hard Limits>\n",
        "\n",
        "  <Show Your Thinking>\n",
        "  After each search tool call, use think_tool to analyze the results:\n",
        "  - What key information did I find?\n",
        "  - What's missing?\n",
        "  - Do I have enough to answer the question comprehensively?\n",
        "  - Should I search more or provide my answer?\n",
        "  </Show Your Thinking>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9fR1uLAPQER"
      },
      "source": [
        "## Tools\n",
        "\n",
        "Thomas already introduced our first tool the `exa_search_and_refine` tool so we will import it from our `tools.py` instead of redefining it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ND5VnRM9Ok-a"
      },
      "outputs": [],
      "source": [
        "# import the  tool Thomas introduced in the previous notebook 01_simple_tool_calling_agent.ipynb\n",
        "from deep_research_bot.tools import exa_search_and_refine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGAdaOuYOk-a"
      },
      "source": [
        "Next we will add 3 new tools to upgrade this agent from a simple search agent to a deep research one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8R4c-hlOk-a"
      },
      "source": [
        "### Clarification tool\n",
        "If you have used another deep research service, like ChatGPT Deep Research, you will be familiar with the first step which is the clarifications questions. Users oftentimes submit a one-sentence request which often lacks the necessary information to provide them a deep answer that will really answer what they were looking for.\n",
        "\n",
        "In the case of ChatGPT, these questions are mandatory and happen every time you create a new Deep Research request, in our case we actually give the agent the choice to call the tool if it thinks it needs more information to get started."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLe18fUeOk-a"
      },
      "outputs": [],
      "source": [
        "@weave.op\n",
        "@function_tool\n",
        "def clarification_tool(clarifying_questions: str) -> str:\n",
        "  \"\"\"Use this tool to ask clarifying questions to the user.\n",
        "\n",
        "  ALWAYS USE THIS TOOL AS SOON AS THE USER SUBMITS A REQUEST. THIS SHOULD BE THE FIRST TOOL CALL.\n",
        "\n",
        "  IMPORTANT: If you can see in the messages history that you have already asked a clarifying question, you almost always do not need to ask another one.\n",
        "\n",
        "  If there are acronyms, abbreviations, or unknown terms, ask the user to clarify.\n",
        "  If you need to ask a question, follow these guidelines:\n",
        "  - Be concise while gathering all necessary information.\n",
        "  - Only ask max 3 questions.\n",
        "  - Make sure to gather all the information needed to carry out the research task in a concise, well-structured manner.\n",
        "  - Use bullet points or numbered lists if appropriate for clarity. Make sure that this uses markdown formatting and will be rendered correctly if the string output is passed to a markdown renderer.\n",
        "  - Don't ask for unnecessary information, or information that the user has already provided. If you can see that the user has already provided the information, do not ask for it again.\n",
        "\n",
        "  This tool will return the user clarifications.\n",
        "  Args:\n",
        "    clarifying_questions: Your questions to the user as a single string. Be concise while gathering all necessary information. Only ask max 3 questions. Use bullet points or numbered lists if appropriate for clarity with markdown formatting. Don't ask for unnecessary information, or information that the user has already provided. If there are acronyms, abbreviations, or unknown terms, ask the user to clarify. This tool will return the user clarifications.\n",
        "  \"\"\"\n",
        "  output = input(clarifying_questions)\n",
        "  return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP9nuRttOk-b"
      },
      "source": [
        "Later on in the evaluations, this tool will be skipped as the benchmark we are using does not accommodate agents asking follow up questions. This is an example where evaluating with generic benchmarks does not fit every use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw2hRZoXOk-b"
      },
      "source": [
        "### Planning tool\n",
        "Another tool we will make available for the agent is the planning tool. The agent should use this tool to analyze the users query and break it down into subqueries.\n",
        "\n",
        "Again we are giving the agent the freedom to use the tool if necessary, however we prompt it and assume it will use the tool.\n",
        "\n",
        "As you can see this tool technically does not return anything when called, it is not using any external service or function. The argument 'plan' that the agent needs to provide is the actual output, when calling a tool the agent fills in the arguments, meaning the action of calling the tool makes the agent come up with the plan. We could return the plan at the end but it would just duplicate the outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58zgUvgb9Qb0"
      },
      "outputs": [],
      "source": [
        "@weave.op\n",
        "@function_tool\n",
        "def planning_tool(plan: str) -> str:\n",
        "  \"\"\"Tool for planning the research.\n",
        "\n",
        "  If there are no clarifying questions, use this tool as the first step of the research.\n",
        "\n",
        "  Args:\n",
        "    plan: A comprehensive research plan as a single string. Include: (1) Short analysis of user request, (2) Sub-queries broken down from the user's request (e.g., for 'what are 3 heaviest pokemons and their weight combined' -> subqueries: 'what are 3 heaviest pokemons', 'pokemon1 weight', 'pokemon2 weight', 'pokemon3 weight'), and (3) Research approach. Format this as structured text within the parameter.\n",
        "  \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvVpUNczOk-b"
      },
      "source": [
        "### Example: using planning tool\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QIPjEDlOk-b"
      },
      "outputs": [],
      "source": [
        "response = call_model(\n",
        "    model_name=MODEL_SMALL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\":\n",
        "        \"\"\"\n",
        "        You are a concert research agent. You research into concert tickets for users.\n",
        "\n",
        "        Tools available:\n",
        "        **planning_tool** : for planning your research\n",
        "        \"\"\"},\n",
        "        {\"role\": \"user\", \"content\": \"Please find tickets for pop concerts in Amsterdam in May-August 2025\"}\n",
        "    ],\n",
        "    tools=[planning_tool.tool_schema]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaKA1_sCOk-b"
      },
      "source": [
        "This tool schema get's inserted in the model's context by the OpenAI client SDK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BlXZpCWOk-b"
      },
      "outputs": [],
      "source": [
        "console.print(planning_tool.tool_schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQa4DUb3Ok-b"
      },
      "outputs": [],
      "source": [
        "console.print(json.loads(response.tool_calls[0].function.arguments)[\"plan\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX9LWPzyOk-c"
      },
      "source": [
        "After you run the above cell, in the `tool_calls` above you can see that the `planning_tool` was called with argument `plan` already."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SvrTciQOk-c"
      },
      "source": [
        "### Think tool\n",
        "The agent should call this tool after each search. This tool will allow the agent to think about the current findings, identify gaps in the research and decide if further research is neccessary.\n",
        "This tool should prevent agent running in loops, researching until it hits the max steps.\n",
        "\n",
        "Putting the reflection as argument and nothing being returned is the same setup as the `planning_tool`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "la2QI_1VPzCG"
      },
      "outputs": [],
      "source": [
        "@weave.op\n",
        "@function_tool\n",
        "def think_tool(reflection: str) -> str:\n",
        "    \"\"\"Tool for strategic reflection on research progress and decision-making.\n",
        "\n",
        "    Use this tool after each search to analyze results and plan next steps systematically.\n",
        "    This creates a deliberate pause in the research workflow for quality decision-making.\n",
        "\n",
        "    When to use:\n",
        "    - After receiving search results: What key information did I find?\n",
        "    - Before deciding next steps: Do I have enough to answer comprehensively?\n",
        "    - When assessing research gaps: What specific information am I still missing?\n",
        "    - Before concluding research: Can I provide a complete answer now?\n",
        "\n",
        "    Reflection should address:\n",
        "    1. Analysis of current findings - What concrete information have I gathered?\n",
        "    2. Gap assessment - What crucial information is still missing?\n",
        "    3. Quality evaluation - Do I have sufficient evidence/examples for a good answer?\n",
        "    4. Strategic decision - Should I continue searching or provide my answer?\n",
        "\n",
        "    Args:\n",
        "        reflection: Your detailed reflection as a single string addressing: (1) Analysis of current findings - What concrete information have I gathered? (2) Gap assessment - What crucial information is still missing? (3) Quality evaluation - Do I have sufficient evidence/examples for a good answer? (4) Strategic decision - Should I continue searching or provide my answer? Use after receiving search results, before deciding next steps, when assessing research gaps, or before concluding research.\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si6BmYXWPs4d"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0aWjRbWOk-c"
      },
      "outputs": [],
      "source": [
        "#we have already covered and created the AgentState class in the previous notebook so lets import it\n",
        "from deep_research_bot.agent import SimpleAgent, AgentState"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd2u5T11PvOx"
      },
      "source": [
        "## Run\n",
        "And now we can run our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8oCz5d7fCMy8"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "\tagent = SimpleAgent(\n",
        "\t\tmodel_name=MODEL_LARGE,\n",
        "\t\tsystem_message=DEEP_RESEARCH_AGENT_PROMPT.format(date=get_today_str()),\n",
        "\t\ttools=[clarification_tool, planning_tool, think_tool, exa_search_and_refine],\n",
        "\t)\n",
        "\tstate = agent.run(user_prompt=\"What type of vegan milk alternative is the healthiest?\")\n",
        "\tconsole.md(f\"Final response: {state.final_assistant_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_5IL_dXexSz"
      },
      "source": [
        "## Context engineering\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCMnWQB2Ok-d"
      },
      "source": [
        "**Context rot**\n",
        "\n",
        "Context rot in LLMs is the degradation of relevance and accuracy in a model‚Äôs responses as its context window fills with outdated, redundant, or tangential information from the ongoing conversation. Even though we are seeing higher and higher context windows in the newer LLMs, the issue of context rot prevails.  \n",
        "\n",
        "There are a few techniques that help manage the context window:\n",
        "- **Compaction**: Summarizing and compressing conversation history to fit within the context window while preserving essential details for continuity.\n",
        "- **Structured note-taking**: Persistently storing key information outside the context window so the agent can recall and build upon it later.\n",
        "- **Sub-agent architectures**: Using specialized sub-agents for focused tasks that return concise summaries to a main coordinating agent, improving efficiency and clarity.\n",
        "\n",
        "In today's session we will create a simple compaction function for our agent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqrKo1ZeOk-d"
      },
      "source": [
        "### Token counting\n",
        "\n",
        "To figure out if the conversation needs compacting we first need to a function to count the tokens.\n",
        "\n",
        "In this simple example we will use a quick estimation method by counting characters and assuming 4 character -> 1 token conversion.\n",
        "\n",
        "For a real token count you could use for example the tiktoken library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0sCz3t5Ok-d"
      },
      "outputs": [],
      "source": [
        "def estimate_token_count(messages: list[dict[str, Any]]) -> int:\n",
        "    \"\"\"\n",
        "    Estimate token count for messages using character-based heuristic. 4 tokens per character.\n",
        "    \"\"\"\n",
        "    total_chars = 0\n",
        "\n",
        "    for message in messages:\n",
        "        # Convert entire message to string and count characters\n",
        "        # This includes role, content, and any other fields\n",
        "        message_str = json.dumps(message)\n",
        "        total_chars += len(message_str)\n",
        "\n",
        "    # Rough heuristic: 4 characters ‚âà 1 token\n",
        "    base_estimate = total_chars / 4\n",
        "\n",
        "    # Add 10% overhead for message formatting\n",
        "    # (things like <|start|>assistant, etc.)\n",
        "    with_overhead = base_estimate * 1.1\n",
        "\n",
        "    return int(with_overhead)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAwo0ip5Ok-d"
      },
      "outputs": [],
      "source": [
        "estimate_token_count([{\"role\": \"user\", \"content\": \"Hello world\"}])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8TX1rldOk-d"
      },
      "source": [
        "### Compaction tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oygv2WWOOk-e"
      },
      "source": [
        "There are two main ways to trigger compaction:\n",
        "1. As a set part of the pipline, every time the conversation goes above 80% -> compact\n",
        "2. Give the compaction function as the tool to agent, pass the token count into context and prompt the agent to keep track and call the compaction tool as needed.\n",
        "\n",
        "We have opted to set a deterministic function in this example.\n",
        "We have created a new `AgentState` where we track the max tokens, estimated tokens already used and the threshold at which we want the compaction to be triggered.\n",
        "\n",
        "The most important function to pay attention to is `compact_conversation` in this function we defined our strategy on how we will summarize the message history.\n",
        "Strategy:\n",
        "1. Keep the system message (always needed)\n",
        "2. Keep the user request (relevant context and request)\n",
        "3. Summarize everything after\n",
        "\n",
        "The summarization happens with an LLM call, to find out the exact instructions review the system prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwNLUU_dOk-e"
      },
      "outputs": [],
      "source": [
        "from pydantic import PrivateAttr, BaseModel\n",
        "\n",
        "class AgentStateCompaction(BaseModel):\n",
        "    \"\"\"Enhanced AgentState with context window management and compaction tracking.\"\"\"\n",
        "    messages: list[dict[str, Any]] = Field(default_factory=list)\n",
        "    step: int = Field(default=0)\n",
        "    final_assistant_content: str | None = None\n",
        "\n",
        "    # the above 3 are the same as in the AgentState class,\n",
        "    # we could have inherited from it but we are using BaseModel to make it easier to understand\n",
        "    max_tokens: int = Field(default=5000)\n",
        "    compaction_count: int = Field(default=0)\n",
        "    compact_model_name: str = Field(default=MODEL_LARGE)\n",
        "    _estimated_tokens: int = PrivateAttr(default=0)\n",
        "    _threshold: float = PrivateAttr(default=0.8)\n",
        "\n",
        "    def model_post_init(self, __context: Any) -> None:\n",
        "        self._estimated_tokens = estimate_token_count(self.messages)\n",
        "        tokens_before = self._estimated_tokens\n",
        "\n",
        "        console.print(f\"Utilization percentage: {self.utilization_percentage()}%\")\n",
        "\n",
        "        if self._estimated_tokens > (self.max_tokens*self._threshold):\n",
        "            console.print(\"Compacting conversation...\")\n",
        "            self.messages = self.compact_conversation()\n",
        "\n",
        "            self._estimated_tokens = estimate_token_count(self.messages)\n",
        "\n",
        "            # Calculate token savings\n",
        "            tokens_after = estimate_token_count(self.messages)\n",
        "            tokens_saved = tokens_before - tokens_after\n",
        "            console.print(f\"   ‚úì Saved {tokens_saved:,} tokens ({tokens_before:,} ‚Üí {tokens_after:,})\")\n",
        "            console.print(f\"Utilization percentage: {self.utilization_percentage()}%\")\n",
        "\n",
        "    def new(self, **updates):\n",
        "        \"Handy method to create a new state with updated values\"\n",
        "        data = self.model_dump()\n",
        "        data.update(updates)\n",
        "        # Re-validate and re-run model_post_init:\n",
        "        return type(self).model_validate(data)\n",
        "\n",
        "    def utilization_percentage(self) -> float:\n",
        "        \"\"\"\n",
        "        Calculate how much of the context window is being used.\n",
        "\n",
        "        Returns:\n",
        "            float: Percentage from 0-100\n",
        "        \"\"\"\n",
        "        if self.max_tokens == 0:\n",
        "            return 0.0\n",
        "        return (self._estimated_tokens / self.max_tokens) * 100\n",
        "\n",
        "\n",
        "    @weave.op(name=\"compact\")\n",
        "    def compact_conversation(self) -> list[dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Compact the conversation by summarizing older messages.\n",
        "        \"\"\"\n",
        "        messages = self.messages\n",
        "\n",
        "        # Preserve: system message (index 0), first message with instructions (index 1)\n",
        "        system_msg = messages[0]\n",
        "        request_msg = messages[1]\n",
        "\n",
        "        # Create a prompt asking for a concise summary\n",
        "        summary_messages =[\n",
        "        {   \"role\": \"system\",\n",
        "            \"content\": \"\"\"You are compacting a deep research agent's conversation history.\n",
        "            Summarize this research conversation history concisely.\n",
        "            Preserve:\n",
        "            - Key findings from web searches (with source URLs if mentioned)\n",
        "            - Important facts, data points, and statistics\n",
        "            - Research decisions and reasoning\n",
        "            - Any identified gaps or areas needing more investigation\"\"\"},\n",
        "        {   \"role\": \"user\",\n",
        "            \"content\": f\"\"\"\n",
        "            Conversation to summarize:\n",
        "            {json.dumps(messages[1:], indent=2)}\n",
        "            Provide a structured, concise summary.\"\"\"\n",
        "        }\n",
        "        ]\n",
        "\n",
        "        # Call the model to generate the summary\n",
        "        summary_response = call_model(\n",
        "            model_name=self.compact_model_name,\n",
        "            messages=summary_messages\n",
        "        )\n",
        "\n",
        "        # Create the compacted message that replaces the old messages\n",
        "        summary_msg = {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": f\"# Compacted conversation summary: \\n\\n{summary_response.content}\"\n",
        "        }\n",
        "\n",
        "        # Build new message history: system + summary + recent messages\n",
        "        new_messages = [system_msg, request_msg, summary_msg]\n",
        "\n",
        "\n",
        "        # Return a new compacted message history\n",
        "        return new_messages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyfwjn_9Ok-e"
      },
      "source": [
        "### Example 1: Dummy cooking assistant\n",
        "This is a dummy example of a set history of messages that will try compaction function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCJtcGWEOk-f"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful cooking assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"how to prepare vegan carbonara ?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"\"\"Here‚Äôs how to make a **delicious vegan carbonara** ‚Äî creamy, smoky, and comforting without any animal products.\n",
        "\n",
        "---\n",
        "\n",
        "### üßÇ Ingredients (2‚Äì3 servings)\n",
        "\n",
        "#### Sauce:\n",
        "\n",
        "* 150 g silken tofu *(or 1 cup unsweetened soy or oat cream)*\n",
        "* 2 tbsp nutritional yeast\n",
        "* 1 tsp Dijon mustard\n",
        "* 1 tbsp olive oil or vegan butter\n",
        "* 1/4 tsp black salt (Kala Namak) ‚Äî gives an eggy flavor\n",
        "* 1/4 tsp turmeric *(optional, for color)*\n",
        "* 1 tsp miso paste *(optional, for depth)*\n",
        "* 2 tbsp plant milk (soy, oat, or almond)\n",
        "* Freshly ground black pepper\n",
        "\n",
        "#### ‚ÄúBacon‚Äù:\n",
        "\n",
        "Choose one:\n",
        "\n",
        "* 100 g smoked tempeh, tofu, or seitan strips\n",
        "* or 4‚Äì5 chopped mushrooms (portobello or king oyster work well)\n",
        "\n",
        "For flavoring:\n",
        "\n",
        "* 1 tbsp soy sauce or tamari\n",
        "* 1/2 tsp liquid smoke *(or smoked paprika)*\n",
        "* 1 tsp maple syrup\n",
        "\n",
        "#### Pasta:\n",
        "\n",
        "* 250 g spaghetti or tagliatelle\n",
        "* Salt for pasta water\n",
        "\n",
        "---\n",
        "\n",
        "### üë©‚Äçüç≥ Instructions\n",
        "\n",
        "1. **Cook the pasta**\n",
        "\n",
        "   * Bring a large pot of salted water to a boil.\n",
        "   * Cook pasta according to package directions until *al dente*.\n",
        "   * Reserve about 1/2 cup of pasta water before draining.\n",
        "\n",
        "2. **Make the ‚Äúbacon‚Äù**\n",
        "\n",
        "   * In a pan, heat a little oil.\n",
        "   * Add your tempeh/tofu/mushrooms and fry until golden.\n",
        "   * Add soy sauce, liquid smoke, and maple syrup.\n",
        "   * Cook for another minute until caramelized and smoky. Set aside.\n",
        "\n",
        "3. **Prepare the sauce**\n",
        "\n",
        "   * Blend all sauce ingredients until smooth and creamy.\n",
        "   * Taste and adjust: add more salt or black salt if you like it more ‚Äúeggy‚Äù.\n",
        "\n",
        "4. **Combine**\n",
        "\n",
        "   * In the same pan, pour the sauce and a splash of pasta water.\n",
        "   * Stir gently over medium heat until thickened (1‚Äì2 min).\n",
        "   * Add cooked pasta and ‚Äúbacon‚Äù, tossing to coat evenly.\n",
        "   * Add more pasta water if the sauce gets too thick.\n",
        "\n",
        "5. **Serve**\n",
        "\n",
        "   * Top with freshly ground black pepper, a sprinkle of nutritional yeast, and chopped parsley if desired.\n",
        "\n",
        "---\n",
        "\n",
        "### üçΩÔ∏è Tips & Variations\n",
        "\n",
        "* **For extra creaminess**: Blend in 1‚Äì2 tbsp cashews (soaked for 15 min in hot water).\n",
        "* **For richness**: Add a touch of vegan parmesan or truffle oil before serving.\n",
        "* **For gluten-free**: Use gluten-free pasta and tamari instead of soy sauce.\n",
        "\n",
        "---\n",
        "\n",
        "Would you like me to tailor it to a specific style ‚Äî e.g., *Italian authentic*, *high-protein*, or *quick 15-minute version*?\"\"\"\n",
        "}\n",
        "    ]\n",
        "\n",
        "state = AgentStateCompaction(messages=messages, max_tokens=600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfNY1BclOk-f"
      },
      "outputs": [],
      "source": [
        "console.print(state.messages[-1][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CsohI_POk-f"
      },
      "source": [
        "### Example 2: DeepResearch agent\n",
        "\n",
        "This is an exact copy of our DeepResearch agent from above, with only changes being the AgentState changed for our new AgentStateCompaction.\n",
        "Now you can run it and see the compaction happening in a real use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hG7NkP95Ok-f"
      },
      "outputs": [],
      "source": [
        "deep_research_agent = SimpleAgent(\n",
        "    model_name=MODEL_SMALL_CONTEXT,\n",
        "    system_message=DEEP_RESEARCH_AGENT_PROMPT.format(date=get_today_str()),\n",
        "    tools=[clarification_tool, planning_tool, think_tool, exa_search_and_refine],\n",
        "    state_class=AgentStateCompaction,\n",
        ")\n",
        "\n",
        "#state = AgentStateCompaction(messages=messages)\n",
        "final_state = deep_research_agent.run(\n",
        "    user_prompt=\"Trace the evolution from Java Servlets to the Spring Boot framework. Explain the problems each iteration aimed to solve, and detail the core functionalities of the Spring framework along with essential knowledge required for developers working with it.\",\n",
        "    max_turns=15,  # Long enough to potentially trigger compaction,\n",
        "\n",
        "    # state kwargs\n",
        "    max_tokens=12_000,\n",
        "    compact_model_name=MODEL_LARGE,\n",
        ")\n",
        "\n",
        "console.print(f\"Final Results:\\n\\n{state.final_assistant_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hc60kLLOk-f"
      },
      "source": [
        "## Evals\n",
        "\n",
        "An important part of creating any LLM powered application is evaluation.\n",
        "\n",
        "You have run evaluation on the SimpleAgent Thomas has introduced at the beginning of the session using the [deep_research_bench](https://github.com/Ayanami0730/deep_research_bench). Here are the results we got as a benchmark:\n",
        "- comprehensivness: 0.29\n",
        "- insight: 0.27\n",
        "- instruction_following: 0.32\n",
        "- overall: 0.29\n",
        "\n",
        "Now we will re-run the same eval on our new DeepResearchAgent and see if we have made any improvements!\n",
        "In our testing we found the metrics improved to:\n",
        "- comprehensivness: 0.39\n",
        "- insight: 0.37\n",
        "- instruction_following: 0.44\n",
        "- overall: 0.40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kNYF6ICOk-f"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to Python path\n",
        "project_root = Path.cwd().parent\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw5nw8mxOk-f"
      },
      "outputs": [],
      "source": [
        "from deep_research_bot.evaluation.eval import run_evaluation\n",
        "from deep_research_bot.evaluation.eval_config import EvalConfig\n",
        "\n",
        "deep_research_agent = SimpleAgent(\n",
        "\t\tmodel_name=MODEL_LARGE,\n",
        "\t\tsystem_message=DEEP_RESEARCH_AGENT_PROMPT.format(date=get_today_str()),\n",
        "\t\ttools=[planning_tool, think_tool, exa_search_and_refine],\n",
        "        state_class=AgentState\n",
        "\t)\n",
        "\n",
        "MAX_TURNS = 10\n",
        "\n",
        "eval_config = EvalConfig(\n",
        "    evaluation_name=f\"DeepResearchAgent_max-turns-{MAX_TURNS}_{agent.model_name.split('/')[-1]}\",\n",
        "    trials=2,\n",
        "    limit=20,\n",
        "    judge_model=\"deepseek-ai/DeepSeek-R1-0528\",\n",
        "    weave_parallelism=4,\n",
        "    queries=project_root / \"data/prompt_data/query.jsonl\",\n",
        "    reference=project_root / \"data/test_data/cleaned_data/reference.jsonl\",\n",
        "    criteria=project_root / \"data/criteria_data/criteria.jsonl\",\n",
        ")\n",
        "\n",
        "results = await run_evaluation(\n",
        "    eval_config=eval_config,\n",
        "    agent_callable=partial(deep_research_agent.run, max_turns=MAX_TURNS, max_tokens=128_000, compact_model_name=MODEL_LARGE),  # <- partial to limit the number of agent turns\n",
        ")\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxLCgsRdOk-f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}